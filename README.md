# Computer vision to recognize construction waste compositions: A novel boundary-aware Transformer (BAT) model
## Usage
There are three part in the proposed semantic segmentation framework: preprocessing, segmentation and postprocessing.
### Preprocessing
Preprocessing logics are saved in `preprocessing/process_label.py`.

### <span id="jump3">Segmentation</span>
**<span id="jump1">1. Dataset building</span>**

- Put your dataset in the path `dataset`, and struct your dataset according the format siminlar 
- Use `preprocessing/gen_train_test_val.py` to split your dataset into train, test and val, and generate corresponding 
txt file.
- Go to `segmentation/local_configs/_base_/datasets/pascal_voc12.py` and change the `data_root` to your own root.

**2. Training**
- Build your own dataset acoording [Dataset building](#jump1).
- Config your model through `args.config` in `segmentation/tools/train.py`.
- Train your model use `segmentation/tools/train.py`.

**3. Evaluation**

Run `segmentation/tools/test.py`, config your checkpoint file by `args.checkpoint`.

### Postprocessing
**1. Generating ground truth for SegFix**
```shell
python segfix/lib/datasets/preprocess/cityscapes/dt_offset_generator.py
```

**<span id="jump2">2. Training</span>**

```shell
bash segfix/scripts/cityscapes/segfix/run_custom_segfix.sh train idx
```
`idx` is log number for builing the output log dir, it should be an integer.

**<span id="jump2">3. Get offset label</span>**

```shell
bash segfix/scripts/cityscapes/segfix/run_custom_segfix.sh segfix_pred_val idx
```
`idx` is log number for builing the output log dir, it should be an integer and the same as the one used in [Training](#jump2).

**<span id="jump5">4. Refinement</span>**

Run `segfix/scripts/cityscapes/segfix.py`.
- `args_input` is the path of your predicted label generated by [Segmentation](#jump3).
- `args_offset` is the offset generated by SegFix in [this step](#jump4).
- `args_out` is the path used to save the refined image.
- `args_read_from_txt` is a txt file save the image file list usd in refinement.

**5. Visualize refinement**

The refined image generated in [refinement](#jump5) only save the category index in each pixel's position, 
you need to run `segfix/vis_and_analyse/vis_refine_out.py` to convert it to RGB image.
the visualized label is saved at `segfix/visualization_out/vis_refine_out`.

**6. Evaluate**

Run `segfix/vis_and_analyse/evaluate.py`

## Reference
1. [https://github.com/NVlabs/SegFormer](https://github.com/NVlabs/SegFormer)
2. [https://github.com/openseg-group/openseg.pytorch](https://github.com/openseg-group/openseg.pytorch).


